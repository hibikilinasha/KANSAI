{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3qL6clGFdzaIqkXkZJgVd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hibikilinasha/KANSAI/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y398ftferjSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a2d181d-91ed-4c1e-f8f4-4d9208c3dcdb"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 2s (2,215 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 145113 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-430{u} \n",
            "0 packages upgraded, 11 newly installed, 1 to remove and 25 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.3 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.3 [68.7 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.3 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 3s (11.0 MB/s)\n",
            "(Reading database ... 145572 files and directories currently installed.)\n",
            "Removing libnvidia-common-430 (430.64-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 145567 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.3) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n",
            "Collecting mecab-python3==0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/e9/bbf5fc790a2bedd96fbaf47a84afa060bfb0b3e0217e5f64b32bd4bbad69/mecab-python3-0.7.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp36-cp36m-linux_x86_64.whl size=155477 sha256=3761464b6a979ad08754bef944f35773d1802b68421c44357b206f1c9eadea14\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/07/3a/5f22ccc9f381f3bc01fa023202061cd1e0e9af855292f005dd\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BHrSDBD-18w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "bdc15c4e-9e1a-468d-c6b4-8c95e6666bdf"
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "import config\n",
        "# For Japanese tokenizer\n",
        "import MeCab\n",
        "# For sanitize dusts in each sentence(original)\n",
        "import sanitize\n",
        "import sys\n",
        "import os\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "#各テキストファイルは、存在してもしなくても自動的に上書きする仕様になっています\n",
        "\n",
        "# The data format\n",
        "#\n",
        "# (A) data/tweets1M.txt\n",
        "#  tweet_get.pyによって作られる、ツイートとリプライのペアが入っているファイル\n",
        "#  奇数行にはツイート、偶数行にはリプライを入れ、２行で１ペアとして保存する\n",
        "#  例)\n",
        "#   1行目: おはよー！今日から新学期だね\n",
        "#   2行目: 今日も一日がんばるぞい！\n",
        "#\n",
        "# 以下は、data_processer.pyの実行によって生成されるファイル\n",
        "#\n",
        "# (B) chatbot_generated/tweets_enc.txt\n",
        "#  tweets1M.txtからツイートだけを集めて保存したファイル\n",
        "#\n",
        "# (C) chatbot_generated/tweets_dec.txt\n",
        "#  tweets1M.txtからリプライだけを集めて保存したファイル\n",
        "#\n",
        "# (D) chatbot_generated/tweets_train_[enc|dec].txt\n",
        "#  学習データとして割り当てられた、ツイートとリプライのペアが入っているファイル\n",
        "#\n",
        "# (E) chatbot_generated/tweets_val_[enc|dec].txt\n",
        "#  テストデータとして割り当てられた、ツイートとリプライのペアが入っているファイル\n",
        "#\n",
        "# (F) chatbot_generated/vocab_enc.txt\n",
        "#  tweets_encに出現したボキャブラリを行毎に保存したファイル\n",
        "#  出現回数の降順に並べている\n",
        "#\n",
        "# (G) chatbot_generated/vocab_dec.txt\n",
        "#  tweets_decに出現したボキャブラリを行毎に保存したファイル\n",
        "#  出現回数の降順に並べている\n",
        "#\n",
        "# (H) chatbot_generated/tweets_[train|val]_[dec|enc]_idx.txt\n",
        "#  tweets_[train|val]_[enc|dec].txtの文章から、\n",
        "#  文章中の単語をid化したものを保存したファイル\n",
        "#\n",
        "\n",
        "texts = [\"tweets_enc\",\"tweets_dec\",\n",
        "         \"tweets_train_enc\", \"tweets_train_dec\",\n",
        "         \"tweets_val_enc\", \"tweets_val_dec\",\n",
        "         \"tweets_train_enc_idx\", \"tweets_train_dec_idx\",\n",
        "         \"tweets_val_enc_idx\", \"tweets_val_dec_idx\",\n",
        "         \"vocab_enc\", \"vocab_dec\"]\n",
        "\n",
        "DIGIT_RE = re.compile(r\"\\d\")\n",
        "\n",
        "_PAD = \"_PAD\"\n",
        "_GO = \"_GO\"\n",
        "_EOS = \"_EOS\"\n",
        "_UNK = \"_UNK\"\n",
        "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
        "\n",
        "PAD_ID = 0\n",
        "GO_ID = 1\n",
        "EOS_ID = 2\n",
        "UNK_ID = 3\n",
        "\n",
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "\n",
        "def japanese_tokenizer(sentence):\n",
        "    assert type(sentence) is str\n",
        "    # Mecab doesn't accept binary, but Python string (utf-8).\n",
        "    result = tagger.parse(sentence)\n",
        "    return result.split()\n",
        "\n",
        "def split_tweets_replies(tweets_path, enc_path, dec_path):\n",
        "    \"\"\"Read data from tweets_paths and split it to tweets and replies.\n",
        "    Args:\n",
        "      tweets_path: original tweets data\n",
        "      enc_path: path to write tweets\n",
        "      dec_path: path to write replies\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    i = 1\n",
        "    with gfile.GFile(tweets_path, mode=\"rb\") as f, gfile.GFile(enc_path, mode=\"w+\") as ef, gfile.GFile(dec_path,\n",
        "                                                                                                       mode=\"w+\") as df:\n",
        "        for line in f:\n",
        "            if not isinstance(line, str):\n",
        "                line = line.decode('utf8')\n",
        "            line = sanitize.sanitize_text(line)\n",
        "\n",
        "            # Odd lines are tweets\n",
        "            if i % 2 == 1:\n",
        "                ef.write(line)\n",
        "                ef.write(\"\\n\")\n",
        "            # Even lines are replies\n",
        "            else:\n",
        "                df.write(line)\n",
        "                df.write(\"\\n\")\n",
        "            i = i + 1\n",
        "\n",
        "def num_lines(file):\n",
        "    \"\"\"Return # of lines in file\n",
        "    Args:\n",
        "      file: Target file.\n",
        "    Returns:\n",
        "      # of lines in file\n",
        "    \"\"\"\n",
        "    return sum(1 for _ in open(file, encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def create_train_validation(source_path, train_path, validation_path, train_ratio=0.75):\n",
        "    \"\"\"Split source file into train and validation data\n",
        "    Args:\n",
        "      source_path: source file path\n",
        "      train_path: Path to write train data\n",
        "      validation_path: Path to write validatio data\n",
        "      train_ratio: Train data ratio\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    nb_lines = num_lines(source_path)\n",
        "    nb_train = int(nb_lines * train_ratio)\n",
        "    counter = 0\n",
        "    with gfile.GFile(source_path, \"r\") as f, gfile.GFile(train_path, \"w\") as tf, gfile.GFile(validation_path,\n",
        "                                                                                             \"w\") as vf:\n",
        "        for line in f:\n",
        "            if counter < nb_train:\n",
        "                tf.write(line)\n",
        "            else:\n",
        "                vf.write(line)\n",
        "            counter = counter + 1\n",
        "\n",
        "\n",
        "# Originally from https://github.com/1228337123/tensorflow-seq2seq-chatbot\n",
        "def sentence_to_token_ids(sentence, vocabulary, tokenizer=japanese_tokenizer, normalize_digits=True):\n",
        "    if tokenizer:\n",
        "        words = tokenizer(sentence)\n",
        "    else:\n",
        "        words = basic_tokenizer(sentence)\n",
        "    if not normalize_digits:\n",
        "        return [vocabulary.get(w, UNK_ID) for w in words]\n",
        "    # Normalize digits by 0 before looking words up in the vocabulary.\n",
        "    # return [vocabulary.get(re.sub(_DIGIT_RE, b\"0\", w), UNK_ID) for w in words] #mark added .decode by Ken\n",
        "    return [vocabulary.get(w, UNK_ID) for w in words]  # added  by Ken\n",
        "\n",
        "\n",
        "# Originally from https://github.com/1228337123/tensorflow-seq2seq-chatbot\n",
        "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
        "                      tokenizer=japanese_tokenizer, normalize_digits=True):\n",
        "    print(\"Tokenizing data in %s\" % data_path)\n",
        "    vocab, _ = initialize_vocabulary(vocabulary_path)\n",
        "    with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
        "        with gfile.GFile(target_path, mode=\"wb\") as tokens_file:  # edit w to wb\n",
        "            counter = 0\n",
        "            for line in data_file:\n",
        "#                line = tf.compat.as_bytes(line)  # added by Ken\n",
        "                counter += 1\n",
        "                if counter % 100000 == 0:\n",
        "                    print(\"  tokenizing line %d\" % counter)\n",
        "                # line is binary here\n",
        "                line = line.decode('utf-8')\n",
        "                token_ids = sentence_to_token_ids(line, vocab, tokenizer,\n",
        "                                                  normalize_digits)\n",
        "                tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
        "\n",
        "\n",
        "# Originally from https://github.com/1228337123/tensorflow-seq2seq-chatbot\n",
        "def initialize_vocabulary(vocabulary_path):\n",
        "    if gfile.Exists(vocabulary_path):\n",
        "        rev_vocab = []\n",
        "        with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
        "            rev_vocab.extend(f.readlines())\n",
        "        rev_vocab = [line.strip() for line in rev_vocab]\n",
        "        # Dictionary of (word, idx)\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "    else:\n",
        "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
        "\n",
        "\n",
        "# From https://github.com/1228337123/tensorflow-seq2seq-chatbot\n",
        "def create_vocabulary(source_path, vocabulary_path, max_vocabulary_size, tokenizer=japanese_tokenizer):\n",
        "    \"\"\"Create vocabulary file. Please see comments in head for file format\n",
        "    Args:\n",
        "      source_path: source file path\n",
        "      vocabulary_path: Path to write vocabulary\n",
        "      max_vocabulary_size: Max vocabulary size\n",
        "      tokenizer: tokenizer used for tokenize each lines\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    with gfile.GFile(source_path, mode=\"r\") as f:\n",
        "        counter = 0\n",
        "        vocab = {}  # (word, word_freq)\n",
        "        for line in f:\n",
        "            counter += 1\n",
        "            words = tokenizer(line)\n",
        "            if counter % 5000 == 0:\n",
        "                sys.stdout.write(\".\")\n",
        "                sys.stdout.flush()\n",
        "            for word in words:\n",
        "                # Normalize numbers. Not sure if it's necessary.\n",
        "                word = re.sub(DIGIT_RE, \"0\", word)\n",
        "                if word in vocab:\n",
        "                    vocab[word] += 1\n",
        "                else:\n",
        "                    vocab[word] = 1\n",
        "        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
        "        if len(vocab_list) > max_vocabulary_size:\n",
        "            vocab_list = vocab_list[:max_vocabulary_size]\n",
        "        with gfile.GFile(vocabulary_path, mode=\"w\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #各ファイルがなければ自動的に生成する\n",
        "    os.chdir(config.GENERATED_DIR)\n",
        "    path = os.getcwd()\n",
        "    for i, text in enumerate(texts):\n",
        "        if os.path.isfile(text) == False:\n",
        "            f = open(text+\".txt\",'w',)\n",
        "            f.close()\n",
        "\n",
        "    print(\"Splitting into tweets and replies...\")\n",
        "    print(\"ツイート数：{}\".format(int(len(open(config.TWEETS_TXT, encoding=\"utf-8\").readlines())/2.0)))\n",
        "    split_tweets_replies(config.TWEETS_TXT, config.TWEETS_ENC_TXT, config.TWEETS_DEC_TXT)\n",
        "    print(\"Done\")\n",
        "\n",
        "    print(\"Splitting into train and validation data...\")\n",
        "    create_train_validation(config.TWEETS_ENC_TXT, config.TWEETS_TRAIN_ENC_TXT, config.TWEETS_VAL_ENC_TXT)\n",
        "    create_train_validation(config.TWEETS_DEC_TXT, config.TWEETS_TRAIN_DEC_TXT, config.TWEETS_VAL_DEC_TXT)\n",
        "    print(\"Done\")\n",
        "\n",
        "    print(\"Creating vocabulary files...\")\n",
        "    create_vocabulary(config.TWEETS_ENC_TXT, config.VOCAB_ENC_TXT, config.MAX_ENC_VOCABULARY)\n",
        "    create_vocabulary(config.TWEETS_DEC_TXT, config.VOCAB_DEC_TXT, config.MAX_DEC_VOCABULARY)\n",
        "    print(\"Done\")\n",
        "\n",
        "    print(\"Creating sentence idx files...\")\n",
        "    data_to_token_ids(config.TWEETS_TRAIN_ENC_TXT, config.TWEETS_TRAIN_ENC_IDX_TXT, config.VOCAB_ENC_TXT)\n",
        "    data_to_token_ids(config.TWEETS_TRAIN_DEC_TXT, config.TWEETS_TRAIN_DEC_IDX_TXT, config.VOCAB_DEC_TXT)\n",
        "    data_to_token_ids(config.TWEETS_VAL_ENC_TXT, config.TWEETS_VAL_ENC_IDX_TXT, config.VOCAB_ENC_TXT)\n",
        "    data_to_token_ids(config.TWEETS_VAL_DEC_TXT, config.TWEETS_VAL_DEC_IDX_TXT, config.VOCAB_DEC_TXT)\n",
        "    print(\"Done\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting into tweets and replies...\n",
            "ツイート数：56445\n",
            "Done\n",
            "Splitting into train and validation data...\n",
            "Done\n",
            "Creating vocabulary files...\n",
            "...........\n",
            "\n",
            "...........\n",
            "\n",
            "Done\n",
            "Creating sentence idx files...\n",
            "Tokenizing data in /content/chatbot_generated/tweets_train_enc.txt\n",
            "Tokenizing data in /content/chatbot_generated/tweets_train_dec.txt\n",
            "Tokenizing data in /content/chatbot_generated/tweets_val_enc.txt\n",
            "Tokenizing data in /content/chatbot_generated/tweets_val_dec.txt\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84zYnKuItSru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "52186eb5-2d9e-43e6-e8e8-07d2c3a46ef8"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "os.chdir('/content')\n",
        "sys.path.append('/content')\n",
        "\n",
        "import config\n",
        "import data_processer\n",
        "import lib.seq2seq_model as seq2seq_model\n",
        "\n",
        "def show_progress(text):\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def read_data_into_buckets(enc_path, dec_path, buckets):\n",
        "    \"\"\"ツイートとリプライを読み込んでその長さに応じたバケツに入れる\n",
        "    引数:\n",
        "      enc_path: ツイートインデックスのpath\n",
        "      dec_path: リプライインデックスのpath\n",
        "      buckets: バケツのリスト（[5,10],[10,15],[20,25]..など）\n",
        "    Returns:\n",
        "      data_set: data_set[i]はbuckets[i]の[tweet,reply]と中身は同じ\n",
        "    \"\"\"\n",
        "    #data_set[i] corresponds data for buckets[i]\n",
        "    data_set = [[] for _ in buckets]\n",
        "    with tf.gfile.GFile(enc_path, mode=\"r\",) as enc_file, tf.gfile.GFile(dec_path, mode=\"r\") as dec_file:\n",
        "        tweet =  enc_file.readline()\n",
        "        reply =  dec_file.readline()\n",
        "        counter = 0\n",
        "        while tweet and reply:\n",
        "            counter += 1\n",
        "            if counter % 100000 == 0:\n",
        "                print(\"  reading data line %d\" % counter)\n",
        "                sys.stdout.flush()\n",
        "            source_ids = [int(x) for x in tweet.split()]\n",
        "            target_ids = [int(x) for x in reply.split()]\n",
        "            target_ids.append(data_processer.EOS_ID)\n",
        "            for bucket_id, (source_size, target_size) in enumerate(buckets):\n",
        "                # Find bucket to put this conversation based on tweet and reply length\n",
        "                if len(source_ids) < source_size and len(target_ids) < target_size:\n",
        "                    data_set[bucket_id].append([source_ids, target_ids])\n",
        "                    break\n",
        "            tweet = enc_file.readline()\n",
        "            reply = dec_file.readline()\n",
        "    for bucket_id in range(len(buckets)):\n",
        "        print(\"{}={}=\".format(buckets[bucket_id], len(data_set[bucket_id])))\n",
        "    return data_set\n",
        "\n",
        "\n",
        "# Originally from https://github.com/1228337123/tensorflow-seq2seq-chatbot\n",
        "#学習させるSeq2Seqモデルを作って保存する\n",
        "def create_or_restore_model(session, buckets, forward_only, beam_search, beam_size):\n",
        "\n",
        "    # beam search is off for training\n",
        "    \"\"\"Create model and initialize or load parameters\"\"\"\n",
        "\n",
        "    model = seq2seq_model.Seq2SeqModel(source_vocab_size=config.MAX_ENC_VOCABULARY,\n",
        "                                       target_vocab_size=config.MAX_DEC_VOCABULARY,\n",
        "                                       buckets=buckets,\n",
        "                                       size=config.LAYER_SIZE,\n",
        "                                       num_layers=config.NUM_LAYERS,\n",
        "                                       max_gradient_norm=config.MAX_GRADIENT_NORM,\n",
        "                                       batch_size=config.BATCH_SIZE,\n",
        "                                       learning_rate=config.LEARNING_RATE,\n",
        "                                       learning_rate_decay_factor=config.LEARNING_RATE_DECAY_FACTOR,\n",
        "                                       beam_search=beam_search,\n",
        "                                       attention=True,\n",
        "                                       forward_only=forward_only,\n",
        "                                       beam_size=beam_size)\n",
        "\n",
        "    print(\"model initialized\")\n",
        "    ckpt = tf.train.get_checkpoint_state(config.GENERATED_DIR)\n",
        "    # the checkpoint filename has changed in recent versions of tensorflow\n",
        "    checkpoint_suffix = \".index\"\n",
        "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path + checkpoint_suffix):\n",
        "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
        "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        print(\"Created model with fresh parameters.\")\n",
        "        session.run(tf.global_variables_initializer())\n",
        "    return model\n",
        "\n",
        "\n",
        "def next_random_bucket_id(buckets_scale):\n",
        "    print(\"output backets scale:{}\".format(buckets_scale))\n",
        "    #0以上1未満の一様乱数を生成\n",
        "    n = np.random.random_sample()\n",
        "    bucket_id = min([i for i in range(len(buckets_scale)) if buckets_scale[i] > n])\n",
        "    return bucket_id\n",
        "\n",
        "#学習させる\n",
        "def train():\n",
        "    \"\"\"\n",
        "GPUで動かすときはこのコードを使用する\n",
        "Only allocate 2/3 of the gpu memory to allow for running gpu-based predictions while training:\n",
        "per_process_gpu_memory_fraction：使用するメモリの最大値を指定する引数\n",
        "1を100%として、0.666は66%のメモリを使用する\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.666)\n",
        "tf_config = tf.ConfigProto(gpu_options=gpu_options)\n",
        "BFC = ベストフィット合体アルゴリズム。\n",
        "dlmallocという安易で高速なメモリ確保の実装方法を使用している\n",
        "tf_config.gpu_options.allocator_type = 'BFC'\n",
        "    \"\"\"\n",
        "\n",
        "    #with tf.Session(config=tf_config) as sess:\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        show_progress(\"Setting up data set for each buckets...\")\n",
        "        train_set = read_data_into_buckets(config.TWEETS_TRAIN_ENC_IDX_TXT, config.TWEETS_TRAIN_DEC_IDX_TXT, config.buckets)\n",
        "        valid_set = read_data_into_buckets(config.TWEETS_VAL_ENC_IDX_TXT, config.TWEETS_VAL_DEC_IDX_TXT, config.buckets)\n",
        "        show_progress(\"done\\n\")\n",
        "\n",
        "        show_progress(\"Creating model...\")\n",
        "        # False for train\n",
        "        beam_search = False\n",
        "        model = create_or_restore_model(sess, config.buckets, forward_only=False, beam_search=beam_search, beam_size=config.beam_size)\n",
        "\n",
        "        show_progress(\"done\\n\")\n",
        "\n",
        "        # list of # of data in ith bucket\n",
        "        train_bucket_sizes = [len(train_set[b]) for b in range(len(config.buckets))]\n",
        "        train_total_size = float(sum(train_bucket_sizes))\n",
        "\n",
        "        # Originally from https://github.com/1228337123/tensorflow-seq2seq-chatbot\n",
        "        # This is for choosing randomly bucket based on distribution\n",
        "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
        "                               for i in range(len(train_bucket_sizes))]\n",
        "\n",
        "        show_progress(\"before train loop\")\n",
        "        # Train Loop\n",
        "        steps = 0\n",
        "        previous_perplexities = []\n",
        "        writer = tf.summary.FileWriter(config.LOGS_DIR, sess.graph)\n",
        "\n",
        "        while True:\n",
        "            bucket_id = next_random_bucket_id(train_buckets_scale)\n",
        "#            print(bucket_id)\n",
        "\n",
        "            # Get batch\n",
        "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
        "            #      show_progress(\"Training bucket_id={0}...\".format(bucket_id))\n",
        "\n",
        "            # Train!\n",
        "#            _, average_perplexity, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
        "#                                                           bucket_id,\n",
        "#                                                           forward_only=False,\n",
        "#                                                           beam_search=beam_search)\n",
        "            _, average_perplexity, summary, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
        "                                                           bucket_id,\n",
        "                                                           forward_only=False,\n",
        "                                                           beam_search=beam_search)\n",
        "\n",
        "            #      show_progress(\"done {0}\\n\".format(average_perplexity))\n",
        "\n",
        "            steps = steps + 1\n",
        "            if steps % 2 == 0:\n",
        "                writer.add_summary(summary, steps)\n",
        "                show_progress(\".\")\n",
        "            if steps % 50 != 0:\n",
        "                continue\n",
        "\n",
        "            # check point\n",
        "            checkpoint_path = os.path.join(config.GENERATED_DIR, \"seq2seq.ckpt\")\n",
        "            show_progress(\"Saving checkpoint...\")\n",
        "            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
        "            show_progress(\"done\\n\")\n",
        "\n",
        "            perplexity = math.exp(average_perplexity) if average_perplexity < 300 else float('inf')\n",
        "            print (\"global step %d learning rate %.4f perplexity \"\n",
        "                   \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
        "\n",
        "            # Decrease learning rate if no improvement was seen over last 3 times.\n",
        "            if len(previous_perplexities) > 2 and perplexity > max(previous_perplexities[-3:]):\n",
        "                sess.run(model.learning_rate_decay_op)\n",
        "            previous_perplexities.append(perplexity)\n",
        "\n",
        "            for bucket_id in range(len(config.buckets)):\n",
        "                if len(valid_set[bucket_id]) == 0:\n",
        "                    print(\"  eval: empty bucket %d\" % bucket_id)\n",
        "                    continue\n",
        "                encoder_inputs, decoder_inputs, target_weights = model.get_batch(valid_set, bucket_id)\n",
        "#                _, average_perplexity, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True, beam_search=beam_search)\n",
        "                _, average_perplexity, valid_summary, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True, beam_search=beam_search)\n",
        "                writer.add_summary(valid_summary, steps)\n",
        "                eval_ppx = math.exp(average_perplexity) if average_perplexity < 300 else float('inf')\n",
        "                print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e41f3525060f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_processer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/data_processer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# For Japanese tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# For sanitize dusts in each sentence(original)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MeCab'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPQXKQGwezdP",
        "colab_type": "code",
        "outputId": "4d908da6-98db-4d09-bc96-328531f5134e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import sys\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/AINGE/')\n",
        "sys.path.append('/content/drive/My Drive/AINGE')\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import train\n",
        "import config\n",
        "import data_processer\n",
        "import random\n",
        "import sanitize\n",
        "\n",
        "def get_prediction(session, model, enc_vocab, rev_dec_vocab, text):\n",
        "    #文章をid化する\n",
        "    token_ids = data_processer.sentence_to_token_ids(text, enc_vocab)\n",
        "    bucket_id = min([b for b in range(len(config.buckets))\n",
        "                     if config.buckets[b][0] > len(token_ids)])\n",
        "    encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)\n",
        "\n",
        "    output_logits = model.step(session, encoder_inputs, decoder_inputs, target_weights, bucket_id, True, beam_search=False)\n",
        "\n",
        "    # np.argmax : 多次元配列（１次元配列も含む）の最大要素をもつインデックスを返す\n",
        "    # 例：a = [[0,1,2,3],[1,2,3,4],[2,3,4,5]] // a[2]が中身の最大要素\n",
        "    # np.argmax(a) // return 2\n",
        "\n",
        "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "    if data_processer.EOS_ID in outputs:\n",
        "        outputs = outputs[:outputs.index(data_processer.EOS_ID)]\n",
        "\n",
        "    # tf.compat.as_str : ファイル名を文字列に変換するメソッド\n",
        "    # vocab_dec.txtのファイル名を文字列に変換する？\n",
        "\n",
        "    text = \"\".join([tf.compat.as_str(rev_dec_vocab[output]) for output in outputs])\n",
        "    print(\"text: \"+ text)\n",
        "    return text\n",
        "\n",
        "#    normal_prediction(bucket_id, decoder_inputs, encoder_inputs, model, rev_dec_vocab, session, target_weights)\n",
        "\n",
        "    #if config.beam_search:\n",
        "#        beam_search_prediction(bucket_id, decoder_inputs, encoder_inputs, model, rev_dec_vocab, session,\n",
        "#                               target_weights)\n",
        "\n",
        "#def normal_prediction(bucket_id, decoder_inputs, encoder_inputs, model, rev_dec_vocab, session, target_weights):\n",
        "#    _, _, output_logits = model.step(session, encoder_inputs, decoder_inputs,\n",
        "#                                     target_weights, bucket_id, True, beam_search=False)\n",
        "#    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "#    if data_processer.EOS_ID in outputs:\n",
        "#        outputs = outputs[:outputs.index(data_processer.EOS_ID)]\n",
        "#    text = \"\".join([tf.compat.as_str(rev_dec_vocab[output]) for output in outputs])\n",
        "#    print(\"Normal Prediction\")\n",
        "#    print(text)\n",
        "\n",
        "\n",
        "def get_beam_serch_prediction(session, model, enc_vocab, rev_dec_vocab, text):\n",
        "    # bucketsの中で最後の要素（最も大きい要素）の入力側のバケツサイズを取得する\n",
        "    max_len = config.buckets[-1][0]\n",
        "    target_text = text\n",
        "    print(text)\n",
        "    # 自分が入力した文章がバケツサイズより長い場合は、バケツサイズに合わせる\n",
        "    if len(text) > max_len:\n",
        "        target_text = text[:max_len]\n",
        "    token_ids = data_processer.sentence_to_token_ids(target_text, enc_vocab)\n",
        "    target_buckets = [b for b in range(len(config.buckets))\n",
        "                      if config.buckets[b][0] > len(token_ids)]\n",
        "    if not target_buckets:\n",
        "        return []\n",
        "\n",
        "#    bucket_id = min(target_buckets)\n",
        "#    とりあえず返答の長さはランダム\n",
        "    bucket_id = random.randint(0,4)\n",
        "    encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)\n",
        "\n",
        "    path, symbol, output_logits = model.step(session, encoder_inputs, decoder_inputs,\n",
        "                                             target_weights, bucket_id, True, beam_search=config.beam_search)\n",
        "    beam_size = config.beam_size\n",
        "    k = output_logits[0]\n",
        "    paths = []\n",
        "    for kk in range(beam_size):\n",
        "        paths.append([])\n",
        "    curr = list(range(beam_size))\n",
        "    num_steps = len(path)\n",
        "    for i in range(num_steps - 1, -1, -1):\n",
        "        for kk in range(beam_size):\n",
        "            paths[kk].append(symbol[i][curr[kk]])\n",
        "            curr[kk] = path[i][curr[kk]]\n",
        "    recos = set()\n",
        "    ret = []\n",
        "    i = 0\n",
        "    for kk in range(beam_size):\n",
        "        foutputs = [int(logit) for logit in paths[kk][::-1]]\n",
        "\n",
        "        # If there is an EOS symbol in outputs, cut them at that point.\n",
        "        if data_processer.EOS_ID in foutputs:\n",
        "            #         # print outputs\n",
        "            foutputs = foutputs[:foutputs.index(data_processer.EOS_ID)]\n",
        "        rec = \"\".join([tf.compat.as_str(rev_dec_vocab[output])for output in foutputs])\n",
        "        if rec not in recos:\n",
        "            recos.add(rec)\n",
        "            print(\"reply {}\".format(i))\n",
        "            i = i + 1\n",
        "            ret.append(rec)\n",
        "    return ret\n",
        "\n",
        "\n",
        "class EasyPredictor:\n",
        "    def __init__(self, session):\n",
        "        self.session = session\n",
        "        train.show_progress(\"Creating model...\")\n",
        "        self.model = train.create_or_restore_model(self.session, config.buckets, forward_only=True, beam_search=config.beam_search, beam_size=config.beam_size)\n",
        "        self.model.batch_size = 1\n",
        "        train.show_progress(\"done\\n\")\n",
        "        self.enc_vocab, _ = data_processer.initialize_vocabulary(config.VOCAB_ENC_TXT)\n",
        "        _, self.rev_dec_vocab = data_processer.initialize_vocabulary(config.VOCAB_DEC_TXT)\n",
        "\n",
        "    def predict(self, text):\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = sanitize.sanitize_text(text)\n",
        "        if config.beam_search:\n",
        "            replies = get_beam_serch_prediction(self.session, self.model, self.enc_vocab, self.rev_dec_vocab, text)\n",
        "            return replies\n",
        "        else:\n",
        "            reply = get_prediction(self.session, self.model, self.enc_vocab, self.rev_dec_vocab, text)\n",
        "            return [reply]\n",
        "\n",
        "\n",
        "def predict():\n",
        "    # GPUメモリの20%を使って会話する\n",
        "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
        "    tf_config = tf.ConfigProto(gpu_options=gpu_options)\n",
        "\n",
        "    with tf.Session(config=tf_config) as sess:\n",
        "\n",
        "        predictor = EasyPredictor(sess)\n",
        "\n",
        "        sys.stdout.write(\"> \")\n",
        "        sys.stdout.flush()\n",
        "        line = sys.stdin.readline()\n",
        "        while line:\n",
        "            replies = predictor.predict(line)\n",
        "#            for i, text in enumerate(replies):\n",
        "#                print(i, text)\n",
        "            print(replies)\n",
        "            print(\"> \", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "            line = sys.stdin.readline()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    predict()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c9c5dd7116bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/AINGE/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/AINGE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/AINGE/'"
          ]
        }
      ]
    }
  ]
}